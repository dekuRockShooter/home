Install Kafka and ZooKeeper [1]:
    $ wget http://apache.claz.org/kafka/1.1.0/kafka_2.11-1.1.0.tgz
    $ tar -xf kafka_2.11-1.1.0.tgz
    $ cd kafka_2.11-1.1.0

    Start ZooKeeper and Kafka.  Output should be a list of log events.
    $ ./bin/zookeeper-server-start.sh config/zookeeper.properties
    $ ./bin/kafka-server-start.sh config/server.properties

Create a topic [1]:
    $ ./bin/kafka-topics.sh --create --zookeeper localhost:2181\
        --replication-factor 1 --partitions 1 --topic test
    # Display a list of all topics.
    # ./bin/kafka-topics.sh --list --zookeeper localhost:2181

Send and receive messages about a topic [1]:
    Execute the following commands in different terminals.

    # Send messages from stdin.
    $ ./bin/kafka-console-producer.sh --broker-list localhost:9092\
        --topic test

    # Receive messages from stdin.  The producer needs to be running.
    $ ./bin/kafka-console-consumer.sh --bootsrap-server localhost:9092\
        --topic test --from-beginning

Set up multiple brokers [1]:
    $ cp config/server.properties config/server-1.properties
    $ cp config/server.properties config/server-2.properties

    Open vim and make the following changes.
    broker.id=1
    listeners=PLAINTEXT://:9093
    log.dir=/tmp/kafka-logs-1

    broker.id=2
    listeners=PLAINTEXT://:9094
    log.dir=/tmp/kafka-logs-2

    Start the brokers.
    $ ./bin/kafka-server-start.sh config/server-1.properties
    $ ./bin/kafka-server-start.sh config/server-2.properties

    Create a replicated topic.
    $ ./bin/kafka-topics.sh --create --zookeeper localhost:2181\
        --replication-factor 3 --partitions 1 --topic replicated_test

    Get information about a topic.  The first line is a summary of the
    partitions.  The following lines display info about each partition.
    $ ./bin/kafka-topics.sh --describe --zookeeper localhost:2181\
        --topic replicated_test

Import and export data [1]:

    Start a connector as a standalone process.  The first config specifies
    the process' behavior, the others specify a connector to create.
    $ cp ../test.txt ./
    $ ./bin/connect-standalone.sh config/connect-standalone.properties\
        config/connect-file-source.properties\
        config/connect-file-sink.properties

    $ cat test.sink.txt
    $ tail -f test.sink.txt
    $ echo 10 20 30 >> test.txt


Lab 2: Stream processing [2]

    Start ZooKeeper and Kafka.
    $ ./bin/zookeeper-server-start.sh config/zookeeper.properties
    $ ./bin/kafka-server-start.sh config/server.properties

    Create input and output topics.
    $ ./bin/kafka-topics.sh --create --zookeeper localhost:2181\
        --replication-factor 1 --partitions 1 --topic streams-plaintext-input
    $ ./bin/kafka-topics.sh --create --zookeeper localhost:2181\
        --replication-factor 1 --partitions 1 --topic streams-wordcount-output

    Start the stream processor.
    $ ./bin/ksfka-run-class.sh\
        org.apache.kafka.streams.examples.wordcount.WordCountDemo

    # Send messages to the input topic from stdin.
    $ ./bin/kafka-console-producer.sh --broker-list localhost:9092\
        --topic streams-plaintext-input
    # Or use a file connector that sends data to streams-plaintext-input.
    $ ./bin/connect-standalone.sh config/connect-standalone.properties\
        config/connect-file-source.properties

    See the processed messages.
    $ ./bin/kafka-console-consumer.sh --bootstrap-server localhost:9092\
        --topic streams-wordcount-output --from-beginning\
        --formatter kafka.tools.DefaultMessageFormatter\
        --property print.key=true\
        --property print.value=true\
        --property key.deserializer=org.apache.kafka.common.serialization.StringDeserialize\
        --property value.deserializer=org.apache.kafka.common.serialization.LongDeserializer

    Create a stream processor from scratch [3].
    $ mvn archetype:generate \
        -DarchetypeGroupId=org.apache.kafka \
        -DarchetypeArtifactId=streams-quickstart-java \
        -DarchetypeVersion=1.1.0 \
        -DgroupId=streams.examples \
        -DartifactId=streams.examples \
        -Dversion=0.1 \
        -Dpackage=myapps

    Start the stream processor [3].
    $ mvn clean package
    $ mvn exec:java -Dexex.mainCLass=myapps.Pipe

    To deploy the processor, start the following processes:
    1) ZooKeeper (section Install Kafka and ZooKeeper)
    2) Kafka (section Install Kafka and ZooKeeper)
    3) A producer/connector that publishes to the stream's input topic (section Import and export data, section Send and receive messages about a topic)
    4) A consumer/connector that subscribes to the stream's output topic (section Import and export data, section Send and receive messages about a topic)
    5) A stream processor (section Stream Processing)


References:
[1] https://kafka.apache.org/quickstart
[2] https://kafka.apache.org/11/documentation/streams/quickstart
[3] https://kafka.apache.org/11/documentation/streams/tutorial
